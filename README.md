# LLM Text Tokenizer

This project demonstrates how to build a simple tokenizer from scratch for large language model (LLM) pretraining. Utilizing Jupyter Notebook, the project entails the following steps:

1. **Downloading Sample Text:** A sample text file is retrieved to serve as the input data.

2. **Tokenization:** The text is tokenized through the application of regular expressions.

3. **Vocabulary Mapping:** A vocabulary mapping is created to recognize special tokens.

4. **Encode/Decode Functionality:** A tokenizer class is developed to implement basic encoding and decoding capabilities, facilitating the transformation of text into token sequences and vice versa.

## Acknowledgements

Inspired by the ["LLMs-from-scratch" series](https://github.com/rasbt/LLMs-from-scratch) by Sebastian Raschka.

